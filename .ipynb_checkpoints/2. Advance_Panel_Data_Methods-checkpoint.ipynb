{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "greatest-cleaner",
   "metadata": {},
   "source": [
    "# Advance Panel Data Methods\n",
    "\n",
    "In this notebook, we look into additional panel data models and methods. We start with the widely used fixed effects (FE) estimator, followed by random effects (RE). The dummy variable regression and correlated random effects approaches can be used as alternatives and generalizations of FE. Finally, we cover robust formulas for the variance-covariance matrix and the implied \"clustered\" standard errors.\n",
    "\n",
    "**Topics:**\n",
    "1. Fixed Effects Estimation\n",
    "2. Random Effects Models\n",
    "3. Dummy Variable Regression and Correlated Random Effects\n",
    "4. Robust (Clustered) Standard Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-steam",
   "metadata": {},
   "source": [
    "## 1. Fixed Effects Estimation\n",
    "\n",
    "We start from the same basic unobserved effects models. Instead of first differencing, we get ride of the unobserved individual effect $a_i$ using the within transfromation:\n",
    "\n",
    "$$y_{it} = \\beta_0 + \\beta_1 x_{it1} + ... + \\beta_k x_{itk} + a_i + u_{it}$$\n",
    "\n",
    "$$\\bar{y}_{i} = \\beta_0 + \\beta_1 \\bar{x}_{i1} + ... + \\beta_k \\bar{x}_{ik} + a_i + \\bar{u}_{i}$$\n",
    "\n",
    "$$\\ddot{y}_{it} = y_{it} - \\bar{y}_{i} = \\beta_1 \\ddot{x}_{it1} + ... + \\beta_k \\ddot{x}_{itk} + \\ddot{u}_{it}$$\n",
    "\n",
    "$$t = 1, 2, 3, ..., T$$\n",
    "\n",
    "$$i = 1, 2, 3, ..., n$$\n",
    "\n",
    "where $\\bar{y}_i$ is the average of $y_{it}$ over time for cross-section unit $i$ and for the other variables accordingly. The within transformation subtracts these individual averages from the respective observations $y_it$.\n",
    "\n",
    "The **fixed effects (FE)** estimator simply estimates the demeaned equation using pooled OLS. Instead of applying the within transformation to all variables and running **ols**, we can simply use **PanelOLS()** in the module **linearmodels**. Demeaning is considered by adding the word **EntityEffects** to the formula. This has the additional advantage that the degrees of freedom are adjusted to the demeaning and the variance-covariance matrix and standard errors of freedom are adjusted to the demeaning and the variance-covariance matrix and standard errors are adjusted accordingly. We will come back to different ways to get the same estimates. \n",
    "\n",
    "### Wooldridge, Example 14.2: Has the Return to Education Changed over Time?\n",
    "\n",
    "We estimate the change of the return to education over time using a fixed effects estimator. The data set *WAGEPAN* is a balanced panel for *n* = 545 individuals over *T* = 8 years. It includes the index variables **nr** and **year** for indivdiuals and years, respectively. Since **educ** does not change over time, we cannot estimate its overall impact and have to use **drop_absorbed = True** in the estimation. However, we can interact it with time dummies to see how the impact changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desperate-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import linearmodels as plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daily-protocol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wagepan' data set\n",
    "wagepan = woo.dataWoo('wagepan')\n",
    "\n",
    "# Create new indices with 'nr' and 'year'\n",
    "wagepan = wagepan.set_index(['nr', 'year'], drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wireless-missouri",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokma\\Anaconda3\\lib\\site-packages\\linearmodels\\panel\\model.py:1798: AbsorbingEffectWarning: \n",
      "Variables have been fully absorbed and have removed from the regression:\n",
      "\n",
      "educ\n",
      "\n",
      "  AbsorbingEffectWarning,\n"
     ]
    }
   ],
   "source": [
    "# FE model estimation\n",
    "reg = plm.PanelOLS.from_formula(\n",
    "    formula = 'lwage ~ married +union + C(year)*educ + EntityEffects',\n",
    "    data = wagepan, drop_absorbed = True)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pacific-lightning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed Effect Regression Table: \n",
      "                        beta      se   t-stat  p-value\n",
      "C(year)[1980]         1.3625  0.0162  83.9031   0.0000\n",
      "C(year)[1981]         1.3400  0.1452   9.2307   0.0000\n",
      "C(year)[1982]         1.3567  0.1451   9.3481   0.0000\n",
      "C(year)[1983]         1.3729  0.1452   9.4561   0.0000\n",
      "C(year)[1984]         1.4468  0.1452   9.9617   0.0000\n",
      "C(year)[1985]         1.4122  0.1451   9.7315   0.0000\n",
      "C(year)[1986]         1.4281  0.1451   9.8404   0.0000\n",
      "C(year)[1987]         1.4529  0.1452  10.0061   0.0000\n",
      "married               0.0548  0.0184   2.9773   0.0029\n",
      "union                 0.0830  0.0194   4.2671   0.0000\n",
      "C(year)[T.1981]:educ  0.0116  0.0123   0.9448   0.3448\n",
      "C(year)[T.1982]:educ  0.0148  0.0123   1.2061   0.2279\n",
      "C(year)[T.1983]:educ  0.0171  0.0123   1.3959   0.1628\n",
      "C(year)[T.1984]:educ  0.0166  0.0123   1.3521   0.1764\n",
      "C(year)[T.1985]:educ  0.0237  0.0123   1.9316   0.0535\n",
      "C(year)[T.1986]:educ  0.0274  0.0123   2.2334   0.0256\n",
      "C(year)[T.1987]:educ  0.0304  0.0123   2.4798   0.0132\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'beta': round(results.params, 4),\n",
    "                     'se': round(results.std_errors, 4),\n",
    "                     't-stat': round(results.tstats, 4),\n",
    "                     'p-value': round(results.pvalues, 4)})\n",
    "\n",
    "print(f'Fixed Effect Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-elite",
   "metadata": {},
   "source": [
    "## 2. Random Effects Models\n",
    "\n",
    "We again base our analysis on the basic unobserved effects model. The **random effect (RE)** model assums that the unobserved effects $a_i$ are independent of (or at least uncorrelated with) the regressors $x_{itj}$ for all *t* and *j* = 1, 2, 3, ..., *k*. Therefore, our main motivation for using **FD** or **FE** disappears: OLS consistently estimates the model parameters under this additional assumption.\n",
    "\n",
    "However, like the situation with heteroscedasticity and autocorrelation, we can obtain more efficient estimates if we take into account the structure of the variances and covariances of the error term. Wooldridge (2019, Section 14.2) shows that the GLS transformation that takes care of their special structure implied by the **RE** model leads to a quasi-demeaned specification\n",
    "\n",
    "$$\\dot{y}_{it} = y_{it} - \\theta \\bar{y}_i = \\beta_0(1 - \\theta) + \\beta_1 \\dot{x}_{it1} + ... + \\beta_k \\dot{x}_{itk} + \\dot{v}_{it}$$\n",
    "\n",
    "where $\\dot{y}_{it}$ is similar to the demeaned $\\ddot{y}_{it}$ presented in the fixed effect equation but subtractsonly a fraction $\\theta$ of the individual averages. The same holds for the regressors $x_{itj}$ and the composite error term $v_{it} = a_i + u_{it}$.\n",
    "\n",
    "The parameter \n",
    "\n",
    "$$\\theta = 1 - \\sqrt{\\frac{\\sigma_u^2}{\\sigma_u^2 + T\\sigma_a^2}}$$\n",
    "\n",
    "depends on the variances of $u_{it}$ and $a_i$and the length of the time series dimention *T*. It is unknown and has to be etimated. Given our experience with **FD** and **FE** estimation, it should not come as a surprise that we can estimate the **RE** model parameters in **linearmodels** using the command **RandomEffects()**. Different versions of estimating the random effects parameter $\\theta$ can be implemented and one version is saved as the attribute **theta** in the results object (see the module [documentation](https://bashtage.github.io/linearmodels/panel/panel/linearmodels.panel.model.RandomEffects.html#linearmodels.panel.model.RandomEffects) for more details).\n",
    "\n",
    "Unlike the **FD** and **FE** estimators, we can include variables in our model that are constant over time for each corss-sectional unit. We can use **pandas** methods to provide a list of these variables as well as of those that do not vary within each point in time.\n",
    "\n",
    "### Wooldridge, Example 14.4: A Wage Equation Using Pandel Data\n",
    "\n",
    "The data set *WAGEPAN* was used in the fixed effects estimation. In this example, we first loads the data set and defines the panel structure. Then, we check the panel dimensions and get a list of time-constant variables using **pandas**. Therefore we calculate grouped variances and used the fact that they are zero over time or individual. With these preparations, we get estimates using OLS, RE, and FE estimatiors. We use **PooledOLS()**, **RandomEffects()**, and **PanelOLS()** (with the option **EntityEffects**), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "polyphonic-aurora",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import linearmodels as plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "featured-smooth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wagepan' data set\n",
    "wagepan = woo.dataWoo('wagepan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "focused-notion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Observation: 4360\n",
      "\n",
      "Total Number of Time Periods: 8\n",
      "\n",
      "Total Number of Individuals: 545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print relevant dimension for panel\n",
    "N = wagepan.shape[0]\n",
    "T = wagepan['year'].drop_duplicates().shape[0]\n",
    "n = wagepan['nr'].drop_duplicates().shape[0]\n",
    "\n",
    "print(f'Total Number of Observation: {N}\\n')\n",
    "print(f'Total Number of Time Periods: {T}\\n')\n",
    "print(f'Total Number of Individuals: {n}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "differential-passion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-varying Variables over Time: \n",
      "Index(['black', 'hisp', 'educ'], dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check non-varying variables\n",
    "\n",
    "# (I) across time and within individuals by calculating individual\n",
    "# specific variance for each variable\n",
    "isv_nr = (wagepan.groupby('nr').var() == 0)  # True, if variance is zero\n",
    "\n",
    "# Choose variables where all grouped variance are zero\n",
    "noVar_nr = isv_nr.all(axis = 0)  # which cols are completely True\n",
    "print(f'Non-varying Variables over Time: \\n{isv_nr.columns[noVar_nr]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "refined-document",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-varying Variable Across Individual: \n",
      "Index(['d81', 'd82', 'd83', 'd84', 'd85', 'd86', 'd87'], dtype='object')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (II) across individuals within one point in time for each variable\n",
    "isv_t = (wagepan.groupby('year').var() == 0)\n",
    "noVar_t = isv_t.all(axis = 0)\n",
    "print(f'Non-varying Variable Across Individual: \\n{isv_nr.columns[noVar_t]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conditional-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new indices by 'nr' and 'year'\n",
    "wagepan = wagepan.set_index(['nr', 'year'], drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "persistent-fusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by First Difference (FD)\n",
    "reg_ols = plm.PooledOLS.from_formula(\n",
    "    formula = 'lwage ~ educ + black + hisp + exper + I(exper**2) +'\n",
    "    'married + union + C(year)', data = wagepan)\n",
    "results_ols = reg_ols.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "exempt-trail",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by Random Effects Model (RE)\n",
    "reg_re = plm.RandomEffects.from_formula(\n",
    "    formula = 'lwage ~ educ + black + hisp + exper + I(exper**2) +'\n",
    "    'married + union + C(year)', data = wagepan)\n",
    "results_re = reg_re.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "known-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate by Fixed Effects Model (FE)\n",
    "reg_fe = plm.PanelOLS.from_formula(\n",
    "    formula = 'lwage ~ I(exper**2) + married + union + C(year) +'\n",
    "    'EntityEffects', data = wagepan)\n",
    "results_fe = reg_fe.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "quick-impression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Theta: 0.6450593029243452\n",
      "\n",
      "FD Regression Table: \n",
      "                 beta      se   t-stat  p-value\n",
      "C(year)[1980]  0.0921  0.0783   1.1761   0.2396\n",
      "C(year)[1981]  0.1504  0.0838   1.7935   0.0730\n",
      "C(year)[1982]  0.1548  0.0893   1.7335   0.0831\n",
      "C(year)[1983]  0.1541  0.0944   1.6323   0.1027\n",
      "C(year)[1984]  0.1825  0.0990   1.8437   0.0653\n",
      "C(year)[1985]  0.2013  0.1031   1.9523   0.0510\n",
      "C(year)[1986]  0.2340  0.1068   2.1920   0.0284\n",
      "C(year)[1987]  0.2659  0.1100   2.4166   0.0157\n",
      "educ           0.0913  0.0052  17.4419   0.0000\n",
      "black         -0.1392  0.0236  -5.9049   0.0000\n",
      "hisp           0.0160  0.0208   0.7703   0.4412\n",
      "exper          0.0672  0.0137   4.9095   0.0000\n",
      "I(exper ** 2) -0.0024  0.0008  -2.9413   0.0033\n",
      "married        0.1083  0.0157   6.8997   0.0000\n",
      "union          0.1825  0.0172  10.6349   0.0000\n",
      "\n",
      "RE Regression Table: \n",
      "                 beta      se  t-stat  p-value\n",
      "C(year)[1980]  0.0234  0.1514  0.1546   0.8771\n",
      "C(year)[1981]  0.0638  0.1601  0.3988   0.6901\n",
      "C(year)[1982]  0.0543  0.1690  0.3211   0.7481\n",
      "C(year)[1983]  0.0436  0.1780  0.2450   0.8065\n",
      "C(year)[1984]  0.0664  0.1871  0.3551   0.7225\n",
      "C(year)[1985]  0.0811  0.1961  0.4136   0.6792\n",
      "C(year)[1986]  0.1152  0.2052  0.5617   0.5744\n",
      "C(year)[1987]  0.1583  0.2143  0.7386   0.4602\n",
      "educ           0.0919  0.0107  8.5744   0.0000\n",
      "black         -0.1394  0.0480 -2.9054   0.0037\n",
      "hisp           0.0217  0.0428  0.5078   0.6116\n",
      "exper          0.1058  0.0154  6.8706   0.0000\n",
      "I(exper ** 2) -0.0047  0.0007 -6.8623   0.0000\n",
      "married        0.0638  0.0168  3.8035   0.0001\n",
      "union          0.1059  0.0179  5.9289   0.0000\n",
      "\n",
      "FE Regression Table: \n",
      "                 beta      se   t-stat  p-value\n",
      "C(year)[1980]  1.4260  0.0183  77.7484   0.0000\n",
      "C(year)[1981]  1.5772  0.0216  72.9656   0.0000\n",
      "C(year)[1982]  1.6790  0.0265  63.2583   0.0000\n",
      "C(year)[1983]  1.7805  0.0333  53.4392   0.0000\n",
      "C(year)[1984]  1.9161  0.0417  45.9816   0.0000\n",
      "C(year)[1985]  2.0435  0.0515  39.6460   0.0000\n",
      "C(year)[1986]  2.1915  0.0630  34.7714   0.0000\n",
      "C(year)[1987]  2.3510  0.0762  30.8669   0.0000\n",
      "I(exper ** 2) -0.0052  0.0007  -7.3612   0.0000\n",
      "married        0.0467  0.0183   2.5494   0.0108\n",
      "union          0.0800  0.0193   4.1430   0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Results\n",
    "\n",
    "theta_hat = results_re.theta.iloc[0, 0]\n",
    "print(f'Estimated Theta: {theta_hat}\\n')\n",
    "\n",
    "table_ols = pd.DataFrame({'beta': round(results_ols.params, 4),\n",
    "                     'se': round(results_ols.std_errors, 4),\n",
    "                     't-stat': round(results_ols.tstats, 4),\n",
    "                     'p-value': round(results_ols.pvalues, 4)})\n",
    "\n",
    "print(f'FD Regression Table: \\n{table_ols}\\n')\n",
    "\n",
    "table_re = pd.DataFrame({'beta': round(results_re.params, 4),\n",
    "                     'se': round(results_re.std_errors, 4),\n",
    "                     't-stat': round(results_re.tstats, 4),\n",
    "                     'p-value': round(results_re.pvalues, 4)})\n",
    "\n",
    "print(f'RE Regression Table: \\n{table_re}\\n')\n",
    "\n",
    "table_fe = pd.DataFrame({'beta': round(results_fe.params, 4),\n",
    "                     'se': round(results_fe.std_errors, 4),\n",
    "                     't-stat': round(results_fe.tstats, 4),\n",
    "                     'p-value': round(results_fe.pvalues, 4)})\n",
    "\n",
    "print(f'FE Regression Table: \\n{table_fe}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-indonesia",
   "metadata": {},
   "source": [
    "The **RE** estimator needs stronger assuptions to be consistent than the **FE** estimator. On the other hand, it is more efficient if these assumptions hold and we can include time constant regressors. A widely used test of this additional assumption is the **Hausman test**. It is based on the comparison between the **FE** and **RE** parameter estimates. We include an example below, which uses the **FE** and **RE** estimates and implements a **Hausman test** as shown in Wooldridge (2010, Section 10.7.3). The null hypothesis that the RE model is consistent is clearly rejected with sensible significant levels like $\\alpha$ = 5% or $\\alpha$ = 1%. It also demonstrates that implementing a test on your own is a lot more cumbersome than relying completely on a module's routines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "sweet-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fitted-robert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the estimated parameters and cov from RE and FE models\n",
    "b_fe = results_fe.params\n",
    "b_fe_cov = results_fe.cov\n",
    "b_re = results_re.params\n",
    "b_re_cov = results_re.cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "political-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Statistic: 43.427071175613065\n",
      "\n",
      "p-value: 9.15061385153848e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hausman Test of FE vs. RE\n",
    "# (I) Find overlapping coefficients\n",
    "common_coef = set(results_fe.params.index).intersection(results_re.params.index)\n",
    "\n",
    "# (II) Calculate differences between FE and RE\n",
    "b_diff = np.array(results_fe.params[common_coef] - results_re.params[common_coef])\n",
    "df = len(b_diff)\n",
    "b_diff.reshape((df, 1))\n",
    "b_cov_diff = np.array(b_fe_cov.loc[common_coef, common_coef] - \n",
    "                      b_re_cov.loc[common_coef, common_coef])\n",
    "b_cov_diff.reshape((df, df))\n",
    "\n",
    "# (III) Calculate test statistic\n",
    "stat = abs(np.transpose(b_diff) @ np.linalg.inv(b_cov_diff) @ b_diff)\n",
    "pval = 1 - stats.chi2.cdf(stat, df)\n",
    "\n",
    "print(f'Test Statistic: {stat}\\n')\n",
    "print(f'p-value: {pval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-chicago",
   "metadata": {},
   "source": [
    "## 3. Dummy Variable Regression and Correlated Random Effects\n",
    "\n",
    "It turns out that we can get the **FE** parameter estimates in two other ways than the within transformation we used previously. The **dummy variable regression** uses OLS on the original variables instead of the transformed ones. But it add *n* - 1 dummy variable (or *n* dummies and removes the constant), one for each cross-sectional unit *i* = 1, 2, 3, ..., *n*. The simplest (although not computationally most efficient) way to implment this in *Python* is to use the cross-sectional index as another categorical variable.\n",
    "\n",
    "The third way to get the same results is the **correlated random effects (CRE)** approach. Instead of assuming that the individual effects $a_i$ are independent of the regressors $x_{itj}$, we assume that they only depend on the averages over time $\\bar{x}_{ij} = \\frac{1}{T} \\sum_{t=1}^{T} x_{itj}$ :\n",
    "\n",
    "$$a_i = \\gamma_0 + \\gamma_1 \\bar{x}_{i1} + ... + \\gamma_k \\bar{x}_{ik} + r_i$$\n",
    "\n",
    "$$y_{it} = \\beta_0 + \\beta_1 x_{it1} + ... + \\beta_k x_{itk} + a_i + u_{it}$$\n",
    "\n",
    "$$y_{it} = \\beta_0 + \\gamma_0 + \\beta_1 x_{it1} + ... + \\beta_k x_{itk} + \\gamma_1 \\bar{x}_{i1} + ... + \\gamma_k \\bar{x}_{ik} + r_i + u_{it}$$\n",
    "\n",
    "If $r_i$ is uncorrelated with the regressors, we consistently estimate the parameters of this model using the **RE** estimator. In addition to the orginal regressors, we include their average over time.\n",
    "\n",
    "We use the *WAGEPAN* data set again to estimate the **FE** parameters using the within transfomration (**reg_we**), the dummy variable approach (**reg_dum**), and the CRE approach (**reg_cre**). We also estimate the **RE** version of this model (**reg_re**). The results confirm that the first three methods deliver exactly the same parameter estimates, while the **RE** estimates differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dangerous-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import linearmodels as plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "vulnerable-modem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wagepan' data set\n",
    "wagepan = woo.dataWoo('wagepan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "supported-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the time ('t') and entity ('entity') columns\n",
    "wagepan['t'] = wagepan['year']\n",
    "wagepan['entity'] = wagepan['nr']\n",
    "\n",
    "# Set 'nr' as the index\n",
    "wagepan = wagepan.set_index(['nr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "helpful-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include group specific means\n",
    "wagepan['married_avg'] = wagepan.groupby('nr').mean()['married']\n",
    "wagepan['union_avg'] = wagepan.groupby('nr').mean()['union']\n",
    "wagepan = wagepan.set_index(['year'], append = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-darwin",
   "metadata": {},
   "source": [
    "**Estimate FE parameters in 3 different ways**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rubber-serve",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lokma\\Anaconda3\\lib\\site-packages\\linearmodels\\panel\\model.py:1798: AbsorbingEffectWarning: \n",
      "Variables have been fully absorbed and have removed from the regression:\n",
      "\n",
      "educ\n",
      "\n",
      "  AbsorbingEffectWarning,\n"
     ]
    }
   ],
   "source": [
    "# 1. Within transformation approach\n",
    "reg_we = plm.PanelOLS.from_formula(\n",
    "    formula = 'lwage ~ married + union + C(t)*educ + EntityEffects',\n",
    "    drop_absorbed = True, data = wagepan)\n",
    "results_we = reg_we.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "digital-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Dummy variables approach\n",
    "reg_dum = smf.ols(\n",
    "    formula = 'lwage ~ married + union + C(t)*educ + C(entity)',\n",
    "    data = wagepan)\n",
    "results_dum = reg_dum.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "complicated-immigration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CRE approach\n",
    "reg_cre = plm.RandomEffects.from_formula(\n",
    "    formula = 'lwage ~ married + union + C(t)*educ + married_avg + union_avg',\n",
    "    data = wagepan)\n",
    "results_cre = reg_cre.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "smoking-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to RE estimate\n",
    "reg_re = plm.RandomEffects.from_formula(\n",
    "    formula = 'lwage ~ married + union + C(t)*educ',\n",
    "    data = wagepan)\n",
    "results_re = reg_re.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "palestinian-kazakhstan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Estimated Parameters from Different Models: \n",
      "                   Within_Trans   Dummy     CRE  Random_Effects\n",
      "married                  0.0548  0.0548  0.0548          0.0773\n",
      "union                    0.0830  0.0830  0.0830          0.1075\n",
      "C(t)[T.1982]:educ        0.0148  0.0148  0.0148          0.0143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "var_selection = ['married', 'union', 'C(t)[T.1982]:educ']\n",
    "\n",
    "table = pd.DataFrame({'Within_Trans': round(results_we.params[var_selection], 4),\n",
    "                     'Dummy': round(results_dum.params[var_selection], 4),\n",
    "                     'CRE': round(results_cre.params[var_selection], 4),\n",
    "                     'Random_Effects': round(results_re.params[var_selection], 4)})\n",
    "\n",
    "print(f'Compare Estimated Parameters from Different Models: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-voluntary",
   "metadata": {},
   "source": [
    "Given we have estiamted the **CRE** model, it is easy to test the null hypothesis that the **RE** estimatimator is consistent. The additional assumptions needed are $\\gamma_1 = ... = \\gamma_k = 0$. they can easily be tested using an *F* test or the very similar **Wald test**. As you see, **linearmodels** conveniently provides the routines for these tests. Like the Hausman test, we clearly reject the null hypothesis that the **RE** model is appropriate with a tiny *p* value of about 0.0001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "activated-helmet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wald Test Results: \n",
      "Linear Equality Hypothesis Test\n",
      "H0: Linear equality constraint is valid\n",
      "Statistic: 19.4058\n",
      "P-value: 0.0001\n",
      "Distributed: chi2(2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RE test as an Wald test on the CRE specific coefficients\n",
    "wtest = results_cre.wald_test(formula = 'married_avg = union_avg = 0')\n",
    "print(f'Wald Test Results: \\n{wtest}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-whole",
   "metadata": {},
   "source": [
    "Another advantage of the **CRE** approach is that we can add time-constant regressors to the model. Since we cannot control for average values $\\bar{x}_{ij}$ for these variables, they have to be uncorrelated with $a_i$ for consistent estimation of *their* coefficients. For the other coefficients of the time-varying variables, we still don't need these additional **RE** assumptions.\n",
    "\n",
    "In the following example, we estimate another version of the wage equation using the **CRE** approach. The variables **married** and **union** vary over time, so we can control for their between effects. The variables **educ**, **black**, **hisp** do not vary. For a causal interpretation of *their* coefficients, we have to rely on uncorrelatedness with $a_i$. Given $a_i$ includes intellignece and other labor market success factors, this uncorrelatedness is more plausible for some variable (like **gender** or **race**) than for other variables (like **education**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "comic-drain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import linearmodels as plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "viral-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wagepan' data set\n",
    "wagepan = woo.dataWoo('wagepan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "temporal-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the time ('t') and entity ('entity') columns\n",
    "wagepan['t'] = wagepan['year']\n",
    "wagepan['entity'] = wagepan['nr']\n",
    "\n",
    "# Set 'nr' as the index\n",
    "wagepan = wagepan.set_index(['nr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "inclusive-possible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include group specific means\n",
    "wagepan['married_avg'] = wagepan.groupby('nr').mean()['married']\n",
    "wagepan['union_avg'] = wagepan.groupby('nr').mean()['union']\n",
    "wagepan = wagepan.set_index(['year'], append = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "worthy-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate CRE parameters\n",
    "reg = plm.RandomEffects.from_formula(\n",
    "    formula = 'lwage ~ married + union + educ + '\n",
    "    'black + hisp + married_avg + union_avg',\n",
    "    data = wagepan)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "legitimate-weight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRE Regression Table: \n",
      "               beta      se   t-stat  p-value\n",
      "married      0.2417  0.0177  13.6772   0.0000\n",
      "union        0.0700  0.0207   3.3804   0.0007\n",
      "educ         0.1257  0.0023  55.4837   0.0000\n",
      "black       -0.0892  0.0499  -1.7864   0.0741\n",
      "hisp         0.0784  0.0426   1.8428   0.0654\n",
      "married_avg -0.0436  0.0450  -0.9685   0.3329\n",
      "union_avg    0.2105  0.0519   4.0576   0.0001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'beta': round(results.params, 4),\n",
    "                     'se': round(results.std_errors, 4),\n",
    "                     't-stat': round(results.tstats, 4),\n",
    "                     'p-value': round(results.pvalues, 4)})\n",
    "\n",
    "print(f'CRE Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-prevention",
   "metadata": {},
   "source": [
    "## 4. Robust (Clustered) Standard Errors\n",
    "\n",
    "We argued above that under the **RE** assumptions, OLS is inefficient but consistent. Instead of using **RE**, we could simply use OLS but would have to adjust the standard errors for the fact that the composite error term $v_{it} = a_i + u_{it}$ is correlated over time because of the constant individual effect $a_i$. In fact, the variance-covariance matrix could be more complex than the **RE** assumption with i.i.d. $u_{it}$ implies. These error terms could be serially correlated and / or heteroscedastic. This would invalidate the standard errors not only of OLS but also of **FD**, **FE**, **RE**, and **CRE**.\n",
    "\n",
    "There is an elegant solution, especially in panels with a large cross-sectional dimension. Similar to standard errors that are robust with respect to heteroscedasticity in cross-sectional data and serial correlation in time series, there are formulas for the variance-covariance matrix for panel data that are robust with respect to heteroscedasticity and *arbitrary* correlations of the error term within a cross-sectional unit (or \"cluster\").\n",
    "\n",
    "These \"clustered\" standard errors are mentioned in Wooldridge (2019, Section 14.4 and Example 13.9). Different versions of the clustered variance-covariance matrix can be computed in **linearmodels**. We repeat the **FD** regression from Example 13.9 and reports the adjusted standard errors. Similar to the heteroscedasticity-robust standard errors, there are different versions of formulas for clustered standard errors. We first use the default type (**results_default**), a clustered type without (**results_cluster**) and with a small sample correction (**results_css**). The latter uses ```debiased = Ture``` (default) to adjust the degrees of freedom when estimating the covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "royal-basketball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import linearmodels as plm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "devoted-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'crime2' data set\n",
    "crime4 = woo.dataWoo('crime4')\n",
    "\n",
    "# Create new indices with county and year\n",
    "crime4 = crime4.set_index(['county', 'year'], drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dedicated-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate FD model\n",
    "reg = plm.FirstDifferenceOLS.from_formula(\n",
    "    formula = 'np.log(crmrte) ~ year + d83 + d84 + d85 + d86 + d87 +'\n",
    "                'lprbarr + lprbconv + lprbpris + lavgsen + lpolpc',\n",
    "            data = crime4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hairy-robertson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with standard SE\n",
    "results_default = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fresh-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with \"clustered\" SE\n",
    "results_cluster = reg.fit(cov_type = 'clustered', cluster_entity = True,\n",
    "                         debiased =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "polar-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression with \"clustered\" SE (small-sample correction)\n",
    "results_css = reg.fit(cov_type = 'clustered', cluster_entity = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "numeric-commission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare SEs from Different Models: \n",
      "           betas  se_default  se_cluster  se_css\n",
      "year      0.0077      0.0171      0.0136  0.0137\n",
      "d83      -0.0999      0.0239      0.0219  0.0222\n",
      "d84      -0.1478      0.0413      0.0356  0.0359\n",
      "d85      -0.1524      0.0584      0.0505  0.0511\n",
      "d86      -0.1249      0.0760      0.0624  0.0630\n",
      "d87      -0.0841      0.0940      0.0773  0.0781\n",
      "lprbarr  -0.3275      0.0300      0.0556  0.0562\n",
      "lprbconv -0.2381      0.0182      0.0390  0.0394\n",
      "lprbpris -0.1650      0.0260      0.0451  0.0456\n",
      "lavgsen  -0.0218      0.0221      0.0254  0.0257\n",
      "lpolpc    0.3984      0.0269      0.1014  0.1025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "table = pd.DataFrame({'betas': round(results_default.params, 4),\n",
    "                     'se_default': round(results_default.std_errors, 4),\n",
    "                     'se_cluster': round(results_cluster.std_errors, 4),\n",
    "                     'se_css': round(results_css.std_errors, 4)})\n",
    "print(f'Compare SEs from Different Models: \\n{table}\\n')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
